{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running make_dicts...\n"
     ]
    }
   ],
   "source": [
    "# Import neccesarry modules\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import json\n",
    "from twitter import *\n",
    "from gender import GenderDetector\n",
    "import os\n",
    "from datetime import *\n",
    "from ethnicity import *\n",
    "import copy\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Import Bokeh Packages\n",
    "from bokeh.layouts import row, column, widgetbox, layout\n",
    "from bokeh.models.widgets import Button, TextInput, Select, Div, DataTable, TableColumn, NumberFormatter, Panel, Tabs\n",
    "from bokeh.models import HoverTool, ColumnDataSource, GMapOptions\n",
    "from bokeh.plotting import show, figure, gmap\n",
    "from bokeh.io import show, push_notebook, output_notebook, reset_output\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application\n",
    "from bokeh.document import Document\n",
    "#pi chart\n",
    "from math import pi\n",
    "import pandas as pd\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#some global variables\n",
    "DEFAULT_PHONE = 'iPhone XS'\n",
    "CHART_COLORS = ['#5ae544', '#e54444']\n",
    "CHART_COLORS_G = ['#42c5f4', '#f441be','#000000']\n",
    "CHART_COLORS_R = ['#cde5cd','#eaf9f9','#960a24','#ca8491','#00a5c8','#85bfc6','#00c8af',\n",
    "                  '#9bcdff','#9bffff','#9abd19','#23788a','#35a2af','#df2740','#132759',\n",
    "                  '#ff0000','#ffff00','#220007','#cc6845','#bc7754','#432c22','#a87057',\n",
    "                  '#373e2e', '#352e3e','#ae6339','#271374','#800000']\n",
    "PADDING = 0.1\n",
    "APP_WIDTH = 650\n",
    "APP_HEIGHT = 700\n",
    "PhoneList = ['iPhone XS','Mate','Galaxy Note','Pixel','oneplus','P20','LG','XPERIA','Galaxy S9']\n",
    "phoneTag = ['iPhone XS','Mate20', 'Galaxy Note8', 'Pixel3', 'Oneplus6T', 'HW P20', 'LG G7','XPERIA','Galaxy S9']\n",
    "phoneTagCopy = []\n",
    "PhoneListCopy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Able to classify tweet to either positive or negative category\n",
    "# Need to use singleton pattern\n",
    "class TweetEmotionClassifier:\n",
    "    def __init__(self):\n",
    "        tweets = [(map(lambda x:x.lower(), pos_tweet.split()), 'pos') for pos_tweet in twitter_samples.strings('positive_tweets.json')] + [(map(lambda x:x.lower(), neg_tweet.split()), 'neg') for neg_tweet in twitter_samples.strings('negative_tweets.json')]\n",
    "        all_words = self._TweetEmotionClassifier__get_all_words()\n",
    "        word_freq_dist = nltk.FreqDist(all_words)\n",
    "        self.features = [word for (word, _) in word_freq_dist.most_common(600)]\n",
    "        training_samples = [(self._TweetEmotionClassifier__extract_features(tweet), category) for (tweet, category) in tweets]\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(training_samples)\n",
    "        \n",
    "    def __get_all_words(self):\n",
    "        pos_words = [word.lower() for pos_tweet in twitter_samples.strings('positive_tweets.json') for word in pos_tweet.split()]\n",
    "        neg_words = [word.lower() for neg_tweet in twitter_samples.strings('negative_tweets.json') for word in neg_tweet.split()]\n",
    "        all_words = pos_words + neg_words\n",
    "        return all_words\n",
    "    \n",
    "    def classify(self, tweet):\n",
    "        featurized_tweet = self._TweetEmotionClassifier__extract_features(map(lambda x:x.lower(), tweet.split()))\n",
    "        return self.classifier.classify(featurized_tweet)\n",
    "        \n",
    "    def __extract_features(self, tweet):\n",
    "        tweet_words = set(tweet)\n",
    "        extracted_features = {}\n",
    "        for feature in self.features:\n",
    "            extracted_features['contains(%s)' % feature] = (feature in tweet_words)\n",
    "        return extracted_features\n",
    "\n",
    "# Tweet data object\n",
    "class Tweet:\n",
    "    def __init__(self, tweet):\n",
    "        self.text = tweet['text']\n",
    "        self.date = tweet['created_at']\n",
    "        self.user_name = tweet['user']['name']\n",
    "        self.user_screen_name = tweet['user']['screen_name']\n",
    "        self.location = tweet['user']['location']\n",
    "        \n",
    "    def get_text(self):\n",
    "        return self.text\n",
    "    \n",
    "    def get_user_name(self):\n",
    "        return self.user_name\n",
    "    \n",
    "    def get_user_screen_name(self):\n",
    "        return self.user_screen_name\n",
    "    \n",
    "    def get_location(self):\n",
    "        return self.location\n",
    "    \n",
    "    def get_date(self):\n",
    "        return self.date\n",
    "    \n",
    "class MobileFeaturesAnalyzer:\n",
    "    def __init__(self, emotion_classifier):\n",
    "        self.emotion_classifier = emotion_classifier\n",
    "        self.mobile_features_lookup_table = json.loads(open(\"feature_keywords.json\", \"r\").read())\n",
    "    \n",
    "    def analyze_single_tweet(self, tweet):\n",
    "        extracted_features = {}\n",
    "        lowered_tweet = tweet.lower()\n",
    "        for feature in self.mobile_features_lookup_table.keys():\n",
    "            for key_word in self.mobile_features_lookup_table[feature]:\n",
    "                if lowered_tweet.find(key_word) != -1:\n",
    "                    keyword_begin = tweet.find(key_word)\n",
    "                    keyword_end = keyword_begin + len(key_word) - 1\n",
    "                    if (True if keyword_begin-1 < 0 else tweet[keyword_begin-1].isalpha()) or (True if keyword_end+1 == len(tweet) else tweet[keyword_end+1].isalpha()):\n",
    "                        continue\n",
    "                    related_sentence = self._MobileFeaturesAnalyzer__extract_keyword_related_sentence(keyword_begin, keyword_end, tweet, key_word)\n",
    "                    if feature not in extracted_features.keys():\n",
    "                        extracted_features[feature] = []\n",
    "                    if related_sentence not in extracted_features[feature]:\n",
    "                        extracted_features[feature].append(related_sentence)\n",
    "        \n",
    "        if not extracted_features:\n",
    "            return extracted_features\n",
    "        \n",
    "        for feature in extracted_features.keys():\n",
    "            pos_count = 0\n",
    "            neg_count = 0\n",
    "            for sentence in extracted_features[feature]:\n",
    "                if self.emotion_classifier.classify(sentence) == 'pos':\n",
    "                    pos_count += 1\n",
    "                if self.emotion_classifier.classify(sentence) == 'neg':\n",
    "                    neg_count += 1\n",
    "            sentence_count = len(extracted_features[feature])\n",
    "            pos_ratio = pos_count/sentence_count\n",
    "            neg_ratio = neg_count/sentence_count\n",
    "            extracted_features[feature] = 'pos' if pos_ratio >= 0.5 else 'neg'\n",
    "        return extracted_features\n",
    "    \n",
    "    def __extract_keyword_related_sentence(self, begin, end, tweet, key_word):\n",
    "        termination_tokens = ['.', ',', '!', '?', '\"']\n",
    "        sentence = key_word\n",
    "        i,j = begin-1,end+1\n",
    "        while True:\n",
    "            if i < 0:\n",
    "                break\n",
    "            if tweet[i] in termination_tokens:\n",
    "                break\n",
    "            sentence = tweet[i] + sentence\n",
    "            i -= 1\n",
    "            \n",
    "        while True:\n",
    "            if j == len(tweet):\n",
    "                break\n",
    "            sentence += tweet[j]\n",
    "            if tweet[j] in termination_tokens:\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "    def feature_statistics(self, tweets):\n",
    "        feature_statistics = {}\n",
    "        for tweet in tweets:\n",
    "            features_analysis = self.analyze_single_tweet(tweet)\n",
    "            for feature in features_analysis.keys():\n",
    "                if feature in feature_statistics:\n",
    "                    feature_statistics[feature] += 1\n",
    "                else:\n",
    "                    feature_statistics[feature] = 1\n",
    "        \n",
    "        return feature_statistics\n",
    "    \n",
    "    def analyze_tweets(self, tweets):\n",
    "        feature_analysis = {}\n",
    "        for tweet in tweets:\n",
    "            single_tweet_features_analysis = self.analyze_single_tweet(tweet)\n",
    "            for feature in single_tweet_features_analysis.keys():\n",
    "                if feature not in feature_analysis:\n",
    "                    feature_analysis[feature] = {'pos': 0, 'neg': 0}\n",
    "                if single_tweet_features_analysis[feature] == 'pos':\n",
    "                    feature_analysis[feature]['pos'] += 1\n",
    "                else:\n",
    "                    feature_analysis[feature]['neg'] += 1\n",
    "        \n",
    "        return feature_analysis\n",
    "    \n",
    "    def most_liked_feature(self, tweets):\n",
    "        return sorted(self.analyze_tweets(tweets).items(), key=lambda x: x[1]['pos']/(x[1]['pos'] + x[1]['neg']), reverse=True)[0][0]\n",
    "    \n",
    "    def most_disliked_feature(self, tweets):\n",
    "        return sorted(self.analyze_tweets(tweets).items(), key=lambda x: x[1]['neg']/(x[1]['pos'] + x[1]['neg']), reverse=True)[0][0]\n",
    "    \n",
    "class Tweets:\n",
    "    def __init__(self, query, size=1000):\n",
    "        credential = json.loads(open(\"credential.json\", \"r\").read())\n",
    "        auth = OAuth(credential['OAUTH_TOKEN'], credential['OAUTH_TOKEN_SECRET'], credential['CONSUMER_KEY'], credential['CONSUMER_SECRET'])\n",
    "        twitter = Twitter(auth=auth)\n",
    "        self.tweets = self._Tweets__fetch_tweets(query, twitter, size)\n",
    "        self.ethnicity_predictor = Ethnicity().make_dicts()\n",
    "        \n",
    "    def __filter_non_informative_tweets(self, tweets):\n",
    "        filtered_tweets = []\n",
    "        MINIMUM_WORD_COUNT = 6\n",
    "        MAXIMUM_SHORT_SENTENCE_COUNT = 2\n",
    "        for tweet in tweets:\n",
    "            tweet_word_count = len(tweet.get_text().split())\n",
    "            short_sentence_count = len([len(sentence) for sentence in tweet.get_text().split('\\n') if len(sentence.split()) < 5 and len(sentence.split()) > 0])\n",
    "            ata_count = len([char for char in tweet.get_text() if char == '@'])\n",
    "            if tweet_word_count < MINIMUM_WORD_COUNT or short_sentence_count > MAXIMUM_SHORT_SENTENCE_COUNT or is_advertisement(tweet.get_text()):\n",
    "                continue\n",
    "            filtered_tweets.append(tweet)\n",
    "        return filtered_tweets\n",
    "    \n",
    "    def __remove_duplicate_tweets(self, tweets):\n",
    "        filtered_tweets = []\n",
    "        tweet_text_set = set()\n",
    "        for tweet in tweets:\n",
    "            if tweet.get_text() not in tweet_text_set:\n",
    "                filtered_tweets.append(tweet)\n",
    "                tweet_text_set.add(tweet.get_text())\n",
    "        return filtered_tweets\n",
    "    \n",
    "    def __fetch_tweets(self, query, twitter, size):\n",
    "        tweets = []\n",
    "        try:\n",
    "            max_id = twitter.search.tweets(q=query,lang='en',count=1)['search_metadata']['max_id']\n",
    "        except:\n",
    "            return tweets\n",
    "        while (len(tweets)<size):\n",
    "            load_size = 100\n",
    "            if size - len(tweets) <= 100:\n",
    "                load_size = size - len(tweets)\n",
    "            try:\n",
    "                tweet_search = twitter.search.tweets(q=query, lang='en',count=load_size, max_id=max_id)\n",
    "            except:\n",
    "                return tweets\n",
    "            number_of_tweets = len(tweet_search['statuses'])\n",
    "            if number_of_tweets == 0:\n",
    "                break\n",
    "            max_id = tweet_search['statuses'][number_of_tweets-1]['id'] - 1\n",
    "            tweets += [Tweet(tweet) for tweet in tweet_search['statuses']]\n",
    "            tweets = self._Tweets__filter_non_informative_tweets(tweets)\n",
    "            tweets = self._Tweets__remove_duplicate_tweets(tweets)\n",
    "        return tweets\n",
    "        \n",
    "    def get_tweets(self):\n",
    "        return self.tweets\n",
    "        \n",
    "    def __predict_gender(self, names, screen_names):\n",
    "        gender_detector = GenderDetector()\n",
    "        predictions = []\n",
    "        names_count = len(names)\n",
    "        for i in range(names_count):\n",
    "            prediction = gender_detector.gender(names[i])\n",
    "            if prediction == None:\n",
    "                prediction = gender_detector.gender(screen_names[i])\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "        return predictions\n",
    "        \n",
    "    def gender_statistics(self):\n",
    "        names = [tweet.get_user_name() for tweet in self.tweets]\n",
    "        screen_names = [tweet.get_user_screen_name() for tweet in self.tweets]\n",
    "        \n",
    "        predictions = self._Tweets__predict_gender(names, screen_names)\n",
    "        stats = {}\n",
    "        total = len(predictions)\n",
    "        male_total, female_total, NA_total = 0, 0, 0\n",
    "        for prediction in predictions:\n",
    "            if prediction == 'm':\n",
    "                male_total += 1\n",
    "            elif prediction == 'f':\n",
    "                female_total += 1\n",
    "            else:\n",
    "                NA_total += 1\n",
    "        stats['male'] = male_total/total\n",
    "        stats['female'] = female_total/total\n",
    "        stats['NA'] = NA_total/total\n",
    "        return stats\n",
    "        \n",
    "    def race_statistics(self):\n",
    "        names = [tweet.get_user_name() for tweet in self.tweets]\n",
    "        predictions = self.ethnicity_predictor.get(names)\n",
    "        stats = {}\n",
    "        total = len(names)\n",
    "        for ethnicity in predictions[\"Ethnicity\"]:\n",
    "            if ethnicity not in stats:\n",
    "                stats[ethnicity] = 1\n",
    "            else:\n",
    "                stats[ethnicity] += 1\n",
    "        return {(k+\"_percentage\"): v/total for k, v in stats.items()}\n",
    "    \n",
    "    def __filter_invalid_location(self, locations):\n",
    "        cities_states_lookup_table = json.loads(open(\"cities_and_states.json\", \"r\").read())\n",
    "        filtered_locations = [location for location in locations if ', ' in location]\n",
    "        filtered_locations = [location for location in filtered_locations if location.split(', ')[1] in cities_states_lookup_table]\n",
    "        filtered_locations = [location for location in filtered_locations if location.split(', ')[0] in cities_states_lookup_table[location.split(', ')[1]]]\n",
    "        return filtered_locations\n",
    "    \n",
    "    def location_statistics(self):\n",
    "        locations = [tweet.get_location() for tweet in self.tweets if tweet.get_location() != '']\n",
    "        non_filtered_locations_count = len(locations)\n",
    "        locations = self._Tweets__filter_invalid_location(locations)\n",
    "        stats = {}\n",
    "        for location in locations:\n",
    "            if location not in stats:\n",
    "                stats[location] = 1\n",
    "            else:\n",
    "                stats[location] += 1\n",
    "        if non_filtered_locations_count != 0:\n",
    "            stats['unknown'] = non_filtered_locations_count - len(locations)\n",
    "        return stats\n",
    "    \n",
    "    def __pos_attitude_percentage(self, attitudes):\n",
    "        attitudes_total = len(attitudes)\n",
    "        pos_total = 0\n",
    "        for attitude in attitudes:\n",
    "            if attitude == 'pos':\n",
    "                pos_total += 1\n",
    "\n",
    "        return pos_total/attitudes_total\n",
    "    \n",
    "    def pos_attitude_percentage(self, emotion_classifier):\n",
    "        attitudes = []\n",
    "        for tweet in self.get_tweets():\n",
    "            attitudes.append(emotion_classifier.classify(tweet.get_text()))\n",
    "\n",
    "        return self._Tweets__pos_attitude_percentage(attitudes)\n",
    "    \n",
    "    def __neg_attitude_percentage(self, attitudes):\n",
    "        attitudes_total = len(attitudes)\n",
    "        neg_total = 0\n",
    "        for attitude in attitudes:\n",
    "            if attitude == 'neg':\n",
    "                neg_total += 1\n",
    "\n",
    "        return neg_total/attitudes_total\n",
    "    \n",
    "    def neg_attitude_percentage(self, emotion_classifier):\n",
    "        attitudes = []\n",
    "        for tweet in self.get_tweets():\n",
    "            attitudes.append(emotion_classifier.classify(tweet.get_text()))\n",
    "\n",
    "        return self._Tweets__neg_attitude_percentage(attitudes)\n",
    "    \n",
    "    def overall_attitude(self, emotion_classifier):\n",
    "        attitudes = []\n",
    "        for tweet in self.get_tweets():\n",
    "            attitudes.append(emotion_classifier.classify(tweet.get_text()))\n",
    "            \n",
    "        pos_percentage = self._Tweets__pos_attitude_percentage(attitudes)\n",
    "        neg_percentage = self._Tweets__neg_attitude_percentage(attitudes)\n",
    "        overall_attitude = ''\n",
    "        if pos_percentage > neg_percentage:\n",
    "            return 'pos'\n",
    "        if neg_percentage > pos_percentage:\n",
    "            return 'neg'\n",
    "        if neg_percentage == pos_percentage:\n",
    "            return 'neutral'\n",
    "        \n",
    "def is_advertisement(tweet):\n",
    "    is_advertisement = False\n",
    "    advertisement_keywords = ['sale', 'sales', 'sell', 'buy', 'blackfriday', 'black friday', 'cyber monday', 'discount', 'discounts', 'ebay', 'amazon', 'best buy', 'bestbuy', 'walmart', 'target', 'used', 'new', 'condition']\n",
    "    for advertisement_keyword in advertisement_keywords:\n",
    "        if advertisement_keyword in tweet.lower():\n",
    "            is_advertisement = True\n",
    "            break\n",
    "    return is_advertisement\n",
    " \n",
    "class CompetingProductFinder:\n",
    "    def __init__(self, product):\n",
    "        #debug\n",
    "        self.CCCC = 0\n",
    "        self.product = product\n",
    "        credential = json.loads(open(\"credential.json\", \"r\").read())\n",
    "        #auth = OAuth(credential['OAUTH_TOKEN_2'], credential['OAUTH_TOKEN_SECRET_2'], credential['CONSUMER_KEY_2'], credential['CONSUMER_SECRET_2'])\n",
    "        auth = OAuth2(bearer_token='AAAAAAAAAAAAAAAAAAAAAJ0p8wAAAAAAL2SRIv1cCuz5B8GtLmTf%2FK88KyA%3DcdNO115mC8dRAkbGgURO3dL2Oj8nA2R2ldNEQRnsJV6ntQBlS3')\n",
    "        self.twitter = Twitter(auth=auth)\n",
    "    \n",
    "    def __filter_non_informative_tweets(self, tweets):\n",
    "        filtered_tweets = []\n",
    "        MINIMUM_WORD_COUNT = 6\n",
    "        MAXIMUM_SHORT_SENTENCE_COUNT = 2\n",
    "        for tweet in tweets:\n",
    "            tweet_word_count = len(tweet.get_text().split())\n",
    "            short_sentence_count = len([len(sentence) for sentence in tweet.get_text().split('\\n') if len(sentence.split()) < 5 and len(sentence.split()) > 0])\n",
    "            ata_count = len([char for char in tweet.get_text() if char == '@'])\n",
    "            if tweet_word_count < MINIMUM_WORD_COUNT or short_sentence_count > MAXIMUM_SHORT_SENTENCE_COUNT or is_advertisement(tweet.get_text()):\n",
    "                continue\n",
    "            filtered_tweets.append(tweet)\n",
    "        return filtered_tweets\n",
    "    \n",
    "    def __fetch_tweets(self, potential_compete_product, d=None):\n",
    "        tweets = []\n",
    "        if d == None:\n",
    "            d = str(date.today())\n",
    "        try:\n",
    "            head_search = self.twitter.search.tweets(q=self.product, until=d, result_type='recent', lang='en')\n",
    "        except:\n",
    "            return tweets\n",
    "        if len(head_search['statuses']) == 0:\n",
    "            return tweets\n",
    "        max_id = head_search['search_metadata']['max_id']\n",
    "        day = int(head_search['statuses'][0]['created_at'][8:10])\n",
    "        break_flag = False\n",
    "        while break_flag == False:\n",
    "            try:\n",
    "                tweet_search = self.twitter.search.tweets(q=\"%s %s\"%(self.product, potential_compete_product), result_type='recent', until=d, lang='en', count=100, max_id=max_id)\n",
    "            except:\n",
    "                #if len(tweet_search['statuses']) == 0:\n",
    "                #    print(\"in exception !!!!!!!!!!!!!!!!\")\n",
    "                #    return tweets\n",
    "                #max_id = tweet_search['statuses'][len(tweet_search['statuses'])-1]['id'] - 1\n",
    "                #for tweet in tweet_search['statuses']:\n",
    "                #    tweet_created_day = int(tweet['created_at'][8:10])\n",
    "                #    if (tweet_created_day != day):\n",
    "                #        break_flag=True\n",
    "                #        break\n",
    "                #   tweets.append(Tweet(tweet))\n",
    "                return tweets\n",
    "            if len(tweet_search['statuses']) == 0:\n",
    "                return tweets\n",
    "            max_id = tweet_search['statuses'][len(tweet_search['statuses'])-1]['id'] - 1\n",
    "            for tweet in tweet_search['statuses']:\n",
    "                tweet_created_day = int(tweet['created_at'][8:10])\n",
    "                if (tweet_created_day != day):\n",
    "                    break_flag=True\n",
    "                    break\n",
    "                tweets.append(Tweet(tweet))\n",
    "        return tweets\n",
    "    \n",
    "    def compete(self, product_to_compare, d=None, filter_tweets=True):\n",
    "        tweets = self._CompetingProductFinder__fetch_tweets(product_to_compare, d=d)\n",
    "        return len(self._CompetingProductFinder__filter_non_informative_tweets(tweets)) if filter_tweets == True else len(tweets)\n",
    "    \n",
    "    def compete_with_multiple_products(self, products_to_compare):\n",
    "        tweets_count_by_day = {product_to_compare:{str(date.today()-timedelta(i+1)):0 for i in range(7)} for product_to_compare in products_to_compare}\n",
    "        advertisment_counts = {product_to_compare:0 for product_to_compare in products_to_compare}\n",
    "        total_tweets_counts = {product_to_compare:0 for product_to_compare in products_to_compare}\n",
    "        date_list=[str(date.today()-timedelta(7-i)) for i in range(7)]\n",
    "        for product_to_compare in products_to_compare:\n",
    "            for i in range(7):\n",
    "                tweets = self._CompetingProductFinder__fetch_tweets(product_to_compare, str(date.today()-timedelta(i)))\n",
    "                tweets_count = len(tweets)\n",
    "                advertisment_count = tweets_count - len(self._CompetingProductFinder__filter_non_informative_tweets(tweets))\n",
    "                total_tweets_counts[product_to_compare] += tweets_count\n",
    "                tweets_count_by_day[product_to_compare][str(date.today()-timedelta(i+1))] = tweets_count\n",
    "                advertisment_counts[product_to_compare] += advertisment_count\n",
    "                \n",
    "        return {\"tweets_count_by_day\": tweets_count_by_day, \"advertisment_counts\": advertisment_counts, \"total_tweets_counts\": total_tweets_counts, \"date_list\":date_list}\n",
    "    \n",
    "def rank_location_by_tweets_count(location_statistics):\n",
    "    sorted_location_statistics = sorted(location_statistics.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [location for (location, tweets_count) in sorted_location_statistics if location != 'unknown']\n",
    "\n",
    "def rank_feature_by_mentions_count(feature_statistics):\n",
    "    sorted_feature_statistics = sorted(feature_statistics.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [feature for (feature, mentions) in sorted_feature_statistics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot all plots\n",
    "def plotTabs(race_distribution, gender_distribution,city_list,pos_percent,neg_percent,attitude,feature_list,liked_feature,disliked_feature,TweetsDic, sumAdDic,sum7day,DateList):\n",
    "    sentiment_distribution = { 'Positive toward target phone': pos_percent, 'Negative toward target phone': neg_percent}\n",
    "    #Feature Rank by Mentioning Count:\n",
    "    rank_list = [i + 1 for i in range(len(feature_list))]\n",
    "\n",
    "    #Location Rank by Tweets Count:\n",
    "    rank_city_list = [i + 1 for i in range(len(city_list))]\n",
    "    \n",
    "    # Spacing ELements\n",
    "    space1 = Div(text='', sizing_mode='scale_height', width=75)\n",
    "    space2 = Div(text='', sizing_mode='scale_height', height=100)\n",
    "    #for height spacing, actually will use it!!!\n",
    "    space3 = Div(text='', sizing_mode='scale_width', height=25)\n",
    "    \n",
    "    #################################### Content for Panel 1 ################################################\n",
    "    tab1_header = Div(text='<div align=\"Left\" style=\"display:block\"><h3>Sentiment Analysis On Target Phone<h3><h5>Below will show the percentage of people who feel positive toward target phone and people who feel negative toward target phone.</h5><br></div>', width=700)    \n",
    "    #pie chart for sentiment\n",
    "    x = sentiment_distribution\n",
    "    data1 = pd.Series(x).reset_index(name='value').rename(columns={'index':'sentiment'})\n",
    "    data1['angle'] = data1['value']/data1['value'].sum() * 2*pi\n",
    "    data1['color'] = CHART_COLORS[:len(x)]\n",
    "    p1 = figure(plot_height=280, title=\"Sentiment Analysis on Target Phone\", toolbar_location=None,tools=\"hover\", tooltips=\"@sentiment: @value\")\n",
    "    p1.wedge(x=0, y=1, radius=0.4,start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),line_color=\"white\", fill_color='color', legend='sentiment', source=data1)\n",
    "    \n",
    "    s1=Div(text='Overall sentiment on targeted phone: ',sizing_mode='scale_height', width=300)\n",
    "    overall_sentiment=Div(text=attitude,sizing_mode='scale_height', width=75)\n",
    "    r1 = row([s1,overall_sentiment],width=200)\n",
    "    #########################################################################################################\n",
    "    \n",
    "    #################################### Content for Panel 2 ################################################\n",
    "    tab2_header = Div(text='<div align=\"Left\" style=\"display:block\"><h3>Trending Features On Target Phone<h3><h5>The trending features of target phone are shown below as well as most liked feature and most disliked feature.</h5><br></div>', width=700)\n",
    "    #table for most talk features\n",
    "    data2 = dict(rank = rank_list,feature = feature_list)\n",
    "    #table for most talk features\n",
    "    source1 = ColumnDataSource(data2)\n",
    "    columns1 = [TableColumn(field=\"feature\",title=\"Trending Features On Target Phone\")]\n",
    "    dt = DataTable(source=source1,columns=columns1,width=300,height=280) \n",
    "    #Most like features and most dislike features\n",
    "    s2 = Div(text='Most Liked Feature: ',sizing_mode='scale_height', width=150)\n",
    "    most_liked_feature=Div(text=liked_feature,sizing_mode='scale_height', width=75)\n",
    "    r2 = row([s2,most_liked_feature],width=150)\n",
    "    s3 = Div(text='Most Disliked Feature: ',sizing_mode='scale_height', width=200)\n",
    "    most_disliked_feature=Div(text=disliked_feature,sizing_mode='scale_height', width=75)\n",
    "    r3 = row([s3,most_disliked_feature],sizing_mode='scale_height', width=200)\n",
    "    c=column([r2,r3],height=100)\n",
    "    #########################################################################################################\n",
    "    \n",
    "    #################################### Content for Panel 3 ################################################\n",
    "    tab3_header = Div(text='<div align=\"Left\" style=\"display:block\"><h3>Location--Gender--Race Distribution<h3><h5>The gender,race,and location distribution of tweets.</h5><br></div>', width=700)\n",
    "    #pie chart for gender distribution\n",
    "    gender = gender_distribution\n",
    "    data3 = pd.Series(gender).reset_index(name='value').rename(columns={'index':'gender'})\n",
    "    data3['angle'] = data3['value']/data3['value'].sum() * 2*pi\n",
    "    data3['color'] = CHART_COLORS_G[:len(gender)]\n",
    "    p2 = figure(plot_height=280, title=\"Gender Distribution\", toolbar_location=None,tools=\"hover\", tooltips=\"@gender: @value\")\n",
    "    p2.wedge(x=0, y=1, radius=0.4,start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),line_color=\"white\", fill_color='color', legend='gender', source=data3)\n",
    "    \n",
    "    #pie chart for race distribution\n",
    "    race = race_distribution\n",
    "    data4 = pd.Series(race).reset_index(name='value').rename(columns={'index':'race'})\n",
    "    data4['angle'] = data4['value']/data4['value'].sum() * 2*pi\n",
    "    data4['color'] = CHART_COLORS_R[:len(race)]\n",
    "    #without tools=\"hover\", to allow drag the piechart\n",
    "    p3 = figure(plot_height=280, title=\"Race Distribution\", toolbar_location=None, tooltips=\"@race: @value\")\n",
    "    p3.wedge(x=0, y=1, radius=0.4,start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),line_color=\"white\", fill_color='color', legend='race', source=data4)\n",
    "    \n",
    "    #datatable for top location\n",
    "    s4=Div(text='Location Distribution Ranked List',sizing_mode='scale_height', width=300)\n",
    "    data5 = dict(rank = rank_city_list,city = city_list)\n",
    "    #table for most talk features\n",
    "    source2 = ColumnDataSource(data5)\n",
    "    columns2 = [TableColumn(field=\"city\",title=\"City\")]\n",
    "    dt2 = DataTable(source=source2,columns=columns2,width=300,height=280)\n",
    "    \n",
    "    c2=column([p2,p3],height=400)\n",
    "    c3=column([c2,space2],height=500)\n",
    "    c4=column([dt2,c3],height=500)\n",
    "    c5=column([s4,c4],height=500)\n",
    "    #########################################################################################################\n",
    "\n",
    "    \n",
    "    #################################### Content for Panel 4 ################################################ \n",
    "    \n",
    "    tab4_header = Div(text='<div align=\"Left\" style=\"display:block\"><h3>Competing Products<h3><h5>Popularity of multiple competing products are shown in the plots below.</h5><br></div>', width=700)\n",
    "\n",
    "    # Set the x_range to the list of categories above\n",
    "    p4 = figure(x_range=phoneTagCopy, plot_height=250, title=\"Targeted Phone Related Tweets Counts In 7days\")\n",
    "\n",
    "    # Categorical values can also be used as coordinates\n",
    "    print(len(PhoneListCopy))\n",
    "    p4.vbar(x=phoneTagCopy, top=[sum7day[PhoneListCopy[i]] for i in range(len(PhoneListCopy))], width=0.5, fill_color='red',legend ='Tweets Number')\n",
    "    p4.vbar(x=phoneTagCopy, top=[sumAdDic[PhoneListCopy[i]] for i in range(len(PhoneListCopy))], width=0.5,legend='Ads Number')\n",
    "    # Set some properties to make the plot look better\n",
    "    p4.xgrid.grid_line_color = None\n",
    "    p4.y_range.start = 0\n",
    "    \n",
    "    \n",
    "    colorList=['aqua', 'black', 'blue', 'fuchsia', 'gray', 'green', 'lime', 'maroon', 'navy']\n",
    "    p5 = figure(plot_width=600, plot_height=500,title=\"Product's Tweets Number\",x_range=DateList)\n",
    "    for i in range(len(PhoneListCopy)):\n",
    "        p5.line(DateList, [TweetsDic[PhoneListCopy[i]][DateList[j]] for j in range(7)], color=colorList[i],line_width=2, alpha=0.5,legend=phoneTagCopy[i])\n",
    "\n",
    "    #########################################################################################################\n",
    "\n",
    "    # Layout Panels\n",
    "    panel1 = Panel(child=column([tab1_header,space1,r1,space3,p1], height=APP_HEIGHT, width=APP_WIDTH), title='Sentiment Analysis On Target Phone')\n",
    "    panel2 = Panel(child=column([tab2_header,c,dt], height=APP_HEIGHT, width=APP_WIDTH), title='Trending Features')\n",
    "    panel3 = Panel(child=column([tab3_header,c5], height=APP_HEIGHT, width=APP_WIDTH), title='Location--Gender--Race')\n",
    "    panel4 = Panel(child=column([tab4_header,p4,p5], height=APP_HEIGHT, width=APP_WIDTH), title='Competing Products')\n",
    "    tabs = Tabs(tabs=[panel1,panel2,panel3,panel4])\n",
    "\n",
    "    return tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bokeh server app\n",
    "def create_doc(doc):  \n",
    "    # Update the server page\n",
    "    def update():\n",
    "        spaceH = Div(text='',sizing_mode='scale_width', height=50)\n",
    "        message1 = Div(text='<div align=\"center\" style=\"display:block\"><h2>Mining tweets from Twitter!</h2><iframe src=\"https://giphy.com/embed/52qtwCtj9OLTi\" width=\"480\" height=\"206\" frameBorder=\"0\"></iframe></div>', width=700)\n",
    "        GUI.children[2] = column([spaceH,message1])\n",
    "        #sleep for 3s to ensure getting data from user\n",
    "        time.sleep(3)\n",
    "        #get input from user\n",
    "        phone = GUI.children[0].children[2].value\n",
    "        #mining tweets and apply nlkt\n",
    "        tweets = Tweets(phone, size=1000)\n",
    "        if len(tweets.get_tweets()) == 0:\n",
    "            spaceH = Div(text='',sizing_mode='scale_width', height=50)\n",
    "            messageE = Div(text='<div align=\"center\" style=\"display:block\"><h2>Rate limit exceeded! Please wait 15min</h2><iframe src=\"https://giphy.com/embed/52qtwCtj9OLTi\" width=\"480\" height=\"206\" frameBorder=\"0\"></iframe></div>', width=700)\n",
    "            GUI.children[2] = column([spaceH,messageE])\n",
    "            return\n",
    "        tweet_emotion_classifier = TweetEmotionClassifier()\n",
    "        mobile_features_analyzer = MobileFeaturesAnalyzer(tweet_emotion_classifier)\n",
    "        feature_statistics = mobile_features_analyzer.feature_statistics([tweet.get_text() for tweet in tweets.get_tweets()])\n",
    "        location_statistics = tweets.location_statistics()\n",
    "        \n",
    "        #sleep for 3s to ensure finish mining tweets\n",
    "        time.sleep(3)\n",
    "        \n",
    "        #analysis data \n",
    "        message2 = Div(text='<div align=\"center\" style=\"display:block\"><h2>Analysing tweets!</h2><iframe src=\"https://giphy.com/embed/52qtwCtj9OLTi\" width=\"480\" height=\"206\" frameBorder=\"0\"></iframe></div>', width=700)\n",
    "        GUI.children[2] = column([spaceH,message2])\n",
    "        race_stat = tweets.race_statistics()\n",
    "        gender_stat = tweets.gender_statistics()\n",
    "        location_stat_rank = rank_location_by_tweets_count(location_statistics)\n",
    "        pos_sentiment = tweets.pos_attitude_percentage(tweet_emotion_classifier)\n",
    "        neg_sentiment = tweets.neg_attitude_percentage(tweet_emotion_classifier)\n",
    "        overall_sentiment = tweets.overall_attitude(tweet_emotion_classifier)\n",
    "        feature_rank_list = rank_feature_by_mentions_count(feature_statistics)\n",
    "        most_like = mobile_features_analyzer.most_liked_feature([tweet.get_text() for tweet in tweets.get_tweets()])\n",
    "        #init competing product object\n",
    "        competing_product_finder = CompetingProductFinder(phone)\n",
    "        most_dislike = mobile_features_analyzer.most_disliked_feature([tweet.get_text() for tweet in tweets.get_tweets()])\n",
    "        global phoneTagCopy\n",
    "        global PhoneListCopy\n",
    "        phoneTagCopy=copy.deepcopy(phoneTag)\n",
    "        PhoneListCopy=copy.deepcopy(PhoneList)\n",
    "        for ph in PhoneList:\n",
    "            if difflib.SequenceMatcher(None, ph.lower(),phone.lower()).quick_ratio() >= 0.8 or (ph.lower() in phone.lower()):\n",
    "                indexx=PhoneList.index(ph)\n",
    "                phoneTagCopy.remove(phoneTagCopy[indexx])\n",
    "                PhoneListCopy.remove(PhoneListCopy[indexx])\n",
    "                break        \n",
    "        \n",
    "        compete_result = competing_product_finder.compete_with_multiple_products(PhoneListCopy)\n",
    "        TweetsDic = compete_result['tweets_count_by_day']\n",
    "        sumAdDic = compete_result['advertisment_counts']\n",
    "        sum7day = compete_result['total_tweets_counts']\n",
    "        DateList = compete_result['date_list']\n",
    "        #sleep for 3s to ensure finish analysising tweets\n",
    "        time.sleep(3)\n",
    "                       \n",
    "        # Display Results\n",
    "        message3 = Div(text='<div align=\"center\" style=\"display:block\"><h2>Plotting results!</h2><iframe src=\"https://giphy.com/embed/52qtwCtj9OLTi\" width=\"480\" height=\"206\" frameBorder=\"0\"></iframe></div>', width=700)\n",
    "        GUI.children[2] = column([spaceH,message2])\n",
    "        time.sleep(3)\n",
    "        #update result GUI\n",
    "        GUI.children[2] = plotTabs(race_stat, gender_stat, location_stat_rank, pos_sentiment, \n",
    "                                   neg_sentiment, overall_sentiment, feature_rank_list, most_like, most_dislike,\n",
    "                                   TweetsDic, sumAdDic,sum7day,DateList)\n",
    "        # reset search menu to default\n",
    "        #GUI.children[0] = buildMenu()\n",
    "        \n",
    "    #build search menu\n",
    "    def buildMenu():\n",
    "        # Buttons\n",
    "        submit = Button(label='Search Phone', button_type='success')\n",
    "        submit.on_click(update)\n",
    "        phoneTag = Div(text='<h3>Phone Model Input</h3>', height=20)\n",
    "        phone_model = TextInput(value=DEFAULT_PHONE, title='Phone Model:',sizing_mode='scale_width')\n",
    "        H1 = Div(text='', height=200)\n",
    "        H2 = Div(text='', height=200)\n",
    "        menu = widgetbox([H1,phoneTag,phone_model,submit,H2], width=200)\n",
    "        return menu\n",
    "            \n",
    "    # Initial Dashboard and Document\n",
    "    seperator = Div(text='', sizing_mode='scale_height', width=75)\n",
    "    spacing = Div(text='', sizing_mode='scale_width', height=50)\n",
    "    intro = Div(text='<div align=\"center\" style=\"display:block\"><h2>Welcome to the MarketingResearch Tool!</h2><h3>Please use the controls in the left menu to begin.</h3></div>', width=700)\n",
    "    p = column([spacing, intro])    \n",
    "    GUI = row([buildMenu(), seperator, p], width=900)\n",
    "    doc.add_root(GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset Output\n",
    "reset_output()\n",
    "output_notebook()\n",
    "# Configure Document\n",
    "handler = FunctionHandler(create_doc)\n",
    "app = Application(handler)\n",
    "doc = app.create_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script src=\"http://localhost:61902/autoload.js?bokeh-autoload-element=1016&bokeh-absolute-url=http://localhost:61902&resources=none\" id=\"1016\"></script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "9838461e0eeb4ee681d4ebc14aa0ae12"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "show(app, new='window',notebook_url=\"localhost:8888\", notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
